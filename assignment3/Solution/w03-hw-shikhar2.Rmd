---
title: "Week 3 - Homework"
author: "STAT 420, Summer 2019, S Khanna, NetID:shikhar2"
date: 'June 1st 2019'
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

***

## Exercise 1 (Using `lm` for Inference)

For this exercise we will use the `cats` dataset from the `MASS` package. You should use `?cats` to learn about the background of this dataset.

**(a)** Fit the following simple linear regression model in `R`. Use heart weight as the response and body weight as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `cat_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.05$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

**Solution 1.a**

```{r}
library(MASS)
data(cats)
cat_model = lm(Hwt ~ Bwt, data=cats)
summary(cat_model)$coefficients[2,c(3,4)]
```

**The null and alternative hypotheses** - Under $H_0$: there is not a significant linear relationship between body weight and heart weight. Under $H_1$: there is a significant linear relationship between body weight and heart weight.

**Value of Test statistic** - The test statistic = 16.1193908 is for testing wether or not $\beta_1$ is zero.

**The p-value of the test** = 6.969045e-34 is to determine to wether or not reject the NULL hypothesis based on the chosen significance level $\alpha$. 

**A statistical decision** at $\alpha = 0.05$ - Based on the given level of alpha, we can easily reject the NULL hypothesis since p_value (6.969045e-34) is significantly less than alpha (0.05).

**Conclusion** - Based on the test above we can say that Bodyweight(Bwt) has a significant linear relationship with Heart Weight (Hwt).


**(b)** Calculate a 90% confidence interval for $\beta_1$. Give an interpretation of the interval in the context of the problem.

**Solution 1.b**

```{r}
confint(cat_model, parm="Bwt", level = 0.90)
```

**Interpretation** -  we are 90% confident that for an increase in Body Weight of 1 kg of cats, the average increase in Heart Weight is between 3.619716 and 4.448409 grams, which is the 90% confidence interval for $β1$.



**(c)** Calculate a 99% confidence interval for $\beta_0$. Give an interpretation of the interval in the context of the problem.

**Solution 1.c**

```{r}
confint(cat_model, parm = "(Intercept)" , level = 0.99)
```


**Interpretation** -  we are 99% confident that the average Heart Weight of cats with Body Weight of 0 kg is between -2.164125 and 1.4508 grams, which is the 99% confidence interval for $β0$, but we don’t really believe that, since we are actually certain that the Weights would always be non-negative.



**(d)** Use a 99% confidence interval to estimate the mean heart weight for body weights of 2.1 and 2.8 kilograms. Which of the two intervals is wider? Why?

**Solution 1.d**

```{r}
predict(cat_model, newdata = data.frame(Bwt = c(2.1, 2.8)), 
        interval = c("confidence"), level = 0.99)
```


**Interpretation** - The interval for 2.1 kg is `r 8.630513 - 7.599225` while for 2.8 kg is `r 11.258630 - 10.618796`. The interval is higher for 2.1 kg since its farther from the Body Weight mean of `r mean(cats$Bwt)`. We also know that the Residual standard error depends on distance between x and mean of x, thus increase in distance increases the RSS which is also the estimation of variation of Noise.



**(e)** Use a 99% prediction interval to predict the heart weight for body weights of 2.8 and 4.2 kilograms.

**Solution 1.e**

```{r}
predict(cat_model, newdata = data.frame(Bwt = c(2.8, 4.2)), 
        interval = c("prediction"), level = 0.99)
```




**(f)** Create a scatterplot of the data. Add the regression line, 90% confidence bands, and 90% prediction bands.

**Solution 1.f**

```{r}
X_grid = seq(min(cats$Bwt), max(cats$Bwt), by = 0.01)
Y_ci_band = predict(cat_model, 
                       newdata = data.frame(Bwt = X_grid), 
                       interval = "confidence", level = 0.90)
Y_pi_band = predict(cat_model, 
                       newdata = data.frame(Bwt = X_grid), 
                       interval = "prediction", level = 0.90) 

plot(Hwt ~ Bwt, data = cats,
     xlab = "Body Weight (in Kg)",
     ylab = "Heart Weight (in gm)",
     main = "Heart Weight vs Body Weight of male and female cats ",
     pch  = 20,
     cex  = 1,
     col  = "grey",
     ylim = c(min(Y_pi_band), max(Y_pi_band)))
abline(cat_model, lwd = 2, col = "darkorange")

lines(X_grid, Y_ci_band[,"lwr"], col = "dodgerblue", lwd = 3, lty = 2)
lines(X_grid, Y_ci_band[,"upr"], col = "dodgerblue", lwd = 3, lty = 2)
lines(X_grid, Y_pi_band[,"lwr"], col = "dodgerblue", lwd = 3, lty = 3)
lines(X_grid, Y_pi_band[,"upr"], col = "dodgerblue", lwd = 3, lty = 3)
points(mean(cats$Bwt), mean(cats$Hwt), pch = "+", cex = 2, col = "black")
```


**(g)** Use a $t$ test to test:

- $H_0: \beta_1 = 4$
- $H_1: \beta_1 \neq 4$

Report the following:

- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.05$

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

**Solution 1.g**

```{r}
beta_1_hat = coef(cat_model)[2]
RSS = summary(cat_model)$sigma
Sxx = sum((cats$Bwt - mean(cats$Bwt))^2)
SE_beta_1_hat = RSS / sqrt(Sxx)

t_val = (beta_1_hat - 4) / SE_beta_1_hat

p_val = 2 * pt(abs(t_val), nrow(cats)-2, lower.tail = FALSE)

c(t_val, p_val)
```

**Interpretation** - Since p-value(0.8919283 ) > alpha(0.05), we fail to reject the NULL hypothesis. 

***

## Exercise 2 (More `lm` for Inference)

For this exercise we will use the `Ozone` dataset from the `mlbench` package. You should use `?Ozone` to learn about the background of this dataset. You may need to install the `mlbench` package. If you do so, do not include code to install the package in your `R` Markdown document.

For simplicity, we will re-perform the data cleaning done in the previous homework.

```{r}
data(Ozone, package = "mlbench")
Ozone = Ozone[, c(4, 6, 7, 8)]
colnames(Ozone) = c("ozone", "wind", "humidity", "temp")
Ozone = Ozone[complete.cases(Ozone), ]
```

**(a)** Fit the following simple linear regression model in `R`. Use the ozone measurement as the response and wind speed as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `ozone_wind_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.01$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

**Solution 2.a**

```{r}
ozone_wind_model = lm(ozone ~ wind, data=Ozone)
summary(ozone_wind_model)$coefficients[2,c(3,4)]
```


**The null and alternative hypotheses** - Under $H_0$: there is not a significant linear relationship between wind speed and Ozone. Under $H_1$: there is a significant linear relationship between wind speed and Ozone.

**Value of Test statistic** - The test statistic = -0.2189811 is for testing wether or not $\beta_1$ is zero.

**The p-value of the test** = 0.8267954  is to determine to wether or not reject the NULL hypothesis based on the chosen significance level $\alpha$. 

**A statistical decision at $\alpha = 0.01$** - Based on the given level of alpha, we can cannot reject the NULL hypothesis since p_value (0.8267954) is higher than alpha (0.01).

**Conclusion** - Based on the test above we do not have enough evidence to reject that there is not a significant linear relationship between wind speed and Ozone.




**(b)** Fit the following simple linear regression model in `R`. Use the ozone measurement as the response and temperature as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `ozone_temp_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.01$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.


**Solution 2.b**

```{r}
ozone_temp_model = lm(ozone ~ temp, data=Ozone)
summary(ozone_temp_model)$coefficients[2,c(3,4)]
```


**The null and alternative hypotheses** - Under $H_0$: there is not a significant linear relationship between temprature and temprature Under $H_1$: there is a significant linear relationship between temprature and Ozone.

**Value of Test statistic** - The test statistic = 2.284896e+01 is for testing wether or not $\beta_1$ is zero.

**The p-value of the test** = 8.153764e-71 is to determine to wether or not reject the NULL hypothesis based on the chosen significance level $\alpha$. 

**A statistical decision at $\alpha = 0.01$** - Based on the given level of alpha, we can reject the NULL hypothesis since p_value (8.153764e-71) is significantly lower than alpha (0.01).

**Conclusion** - Based on the test above we can conclude that there is a a significant linear relationship between Ozone and temprature.



***

## Exercise 3 (Simulating Sampling Distributions)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = -5$
- $\beta_1 = 3.25$
- $\sigma^2 = 16$

We will use samples of size $n = 50$.

**(a)** Simulate this model $2000$ times. Each time use `lm()` to fit a simple linear regression model, then store the value of $\hat{\beta}_0$ and $\hat{\beta}_1$. Set a seed using **your** birthday before performing the simulation. Note, we are simulating the $x$ values once, and then they remain fixed for the remainder of the exercise.

**Solution 3.a**

```{r}
birthday = 19820517
set.seed(birthday)
n = 50
x = seq(0, 10, length = n)

beta_0=-5
beta_1=3.25
sigma = 4

num_samples = 2000
beta_0_hats = rep(0, num_samples)
beta_1_hats = rep(0, num_samples)

for (i in 1:num_samples) {
  eps = rnorm(n, mean = 0, sd = sigma)
  y   = beta_0 + beta_1 * x + eps
  
  sim_model = lm(y ~ x)
  
  beta_0_hats[i] = coef(sim_model)[1]
  beta_1_hats[i] = coef(sim_model)[2]
}


```

**(b)** Create a table that summarizes the results of the simulations. The table should have two columns, one for $\hat{\beta}_0$ and one for $\hat{\beta}_1$. The table should have four rows:

- A row for the true expected value given the known values of $x$
- A row for the mean of the simulated values
- A row for the true standard deviation given the known values of $x$
- A row for the standard deviation of the simulated values

**Solution 3.b**

```{r}

Sxx = sum( (x-mean(x))^2 )
SE_beta_0_hat = sigma * sqrt((1/n + mean(x)^2/Sxx))
SE_beta_1_hat = sigma / sqrt(Sxx)

results = data.frame(
  Measure = c("True Expected Value", "Meam of Simulated", "True SD", "SD from Simulated"),
  Beta_0_hat = c(-5, mean(beta_0_hats), SE_beta_0_hat, sd(beta_0_hats)),
  beta_1_hat = c(3.25, mean(beta_1_hats), SE_beta_1_hat, sd(beta_1_hats))
)
knitr::kable(results)
```




**(c)** Plot two histograms side-by-side:

- A histogram of your simulated values for $\hat{\beta}_0$. Add the normal curve for the true sampling distribution of $\hat{\beta}_0$.
- A histogram of your simulated values for $\hat{\beta}_1$. Add the normal curve for the true sampling distribution of $\hat{\beta}_1$.

**Solution 3.c**

```{r}

par(mfrow=c(1,2))
hist(beta_0_hats, prob = TRUE, breaks = 20, 
     xlab = expression(hat(beta)[0]), main = "", border = "dodgerblue")
curve(dnorm(x, mean = beta_0, sd = SE_beta_0_hat), 
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_1_hats, prob = TRUE, breaks = 20, 
     xlab = expression(hat(beta)[1]), main = "", border = "grey")
curve(dnorm(x, mean = beta_1, sd = SE_beta_1_hat), 
      col = "yellow", add = TRUE, lwd = 3)
```


***

## Exercise 4 (Simulating Confidence Intervals)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = 5$
- $\beta_1 = 2$
- $\sigma^2 = 9$

We will use samples of size $n = 25$.

Our goal here is to use simulation to verify that the confidence intervals really do have their stated confidence level. Do **not** use the `confint()` function for this entire exercise.

**(a)** Simulate this model $2500$ times. Each time use `lm()` to fit a simple linear regression model, then store the value of $\hat{\beta}_1$ and $s_e$. Set a seed using **your** birthday before performing the simulation. Note, we are simulating the $x$ values once, and then they remain fixed for the remainder of the exercise.

```{r}
birthday = 19820501
set.seed(birthday)
n = 25
x = seq(0, 2.5, length = n)
```

**Solution 4.a**

```{r}
beta_0=5
beta_1=2
sigma=3

num_samples = 2500
beta_0_hats = rep(0, num_samples)
beta_1_hats = rep(0, num_samples)
SE_beta_0_hats = rep(0, num_samples)
SE_beta_1_hats = rep(0, num_samples)
Sxx = sum( (x - mean(x))^2 )

for (i in 1:num_samples) {
  eps = rnorm(n, mean = 0, sd = sigma)
  y   = beta_0 + beta_1 * x + eps
  
  sim_model = lm(y ~ x)
  
  beta_0_hats[i] = coef(sim_model)[1]
  beta_1_hats[i] = coef(sim_model)[2]
  SE_beta_0_hats[i] = summary(sim_model)$sigma * sqrt(1/n + mean(x)^2/Sxx)
  SE_beta_1_hats[i] = summary(sim_model)$sigma * sqrt(1/Sxx)
  
}
```



**(b)** For each of the $\hat{\beta}_1$ that you simulated, calculate a 95% confidence interval. Store the lower limits in a vector `lower_95` and the upper limits in a vector `upper_95`. Some hints:

- You will need to use `qt()` to calculate the critical value, which will be the same for each interval.
- Remember that `x` is fixed, so $S_{xx}$ will be the same for each interval.
- You could, but do not need to write a `for` loop. Remember vectorized operations.

**Solution 4.b**

```{r}
crit = qt((1-0.95)/2, df=n-2, lower.tail = FALSE)

lower_95 = beta_1_hats - crit * SE_beta_1_hats
upper_95 = beta_1_hats + crit * SE_beta_1_hats

```


**(c)** What proportion of these intervals contains the true value of $\beta_1$?

**Solution 4.c**

```{r}
mean(lower_95 < 2 & 2 < upper_95)
```


**(d)** Based on these intervals, what proportion of the simulations would reject the test $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$ at $\alpha = 0.05$?

**Solution 4.d**

```{r}
1 - mean(lower_95 < 0 & 0 < upper_95)
```


**(e)** For each of the $\hat{\beta}_1$ that you simulated, calculate a 99% confidence interval. Store the lower limits in a vector `lower_99` and the upper limits in a vector `upper_99`.

**Solution 4.e**

```{r}
crit = qt((1-0.99)/2, df=n-2, lower.tail = FALSE)

lower_99 = beta_1_hats - crit * SE_beta_1_hats
upper_99 = beta_1_hats + crit * SE_beta_1_hats

```



**(f)** What proportion of these intervals contains the true value of $\beta_1$?

**Solution 4.f**

```{r}
mean(lower_99 < 2 & 2 < upper_99)
```


**(g)** Based on these intervals, what proportion of the simulations would reject the test $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$ at $\alpha = 0.01$?

**Solution 4.g**

```{r}
1 - mean(lower_99 < 0 & 0 < upper_99)
```


***

## Exercise 5 (Prediction Intervals "without" `predict`)

Write a function named `calc_pred_int` that performs calculates prediction intervals:

$$
\hat{y}(x) \pm t_{\alpha/2, n - 2} \cdot s_e\sqrt{1 + \frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}}}.
$$

for the linear model

$$
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i.
$$

**(a)** Write this function. You may use the `predict()` function, but you may **not** supply a value for the `level` argument of `predict()`. (You can certainly use `predict()` any way you would like in order to check your work.)

The function should take three inputs:

- `model`, a model object that is the result of fitting the SLR model with `lm()`
- `newdata`, a data frame with a single observation (row)
    - This data frame will need to have a variable (column) with the same name as the data used to fit `model`.
- `level`, the level (0.90, 0.95, etc) for the interval with a default value of `0.95`

The function should return a named vector with three elements:

- `estimate`, the midpoint of the interval
- `lower`, the lower bound of the interval
- `upper`, the upper bound of the interval

**Solution 5.a**



```{r}
calc_pred_int = function(model,newdata,level=0.95){

data = newdata
n = nrow(model$model)
Se = summary(model)$sigma
x = model$model[,2]

estimate = predict(model, newdata=data)
crit = qt((1-level)/2, df=n-2, lower.tail = FALSE)
Sxx = sum((x - mean(x))^2)

g = sqrt(1 + 1/n + (data$Bwt -mean(x))^2/Sxx)
lower = estimate - crit * Se * g
upper = estimate + crit * Se * g

v = c(estimate, lower, upper)
names(v) = c("estimate", "lower", "upper")

return(v)
}

```


**(b)** After writing the function, run this code:

```{r}
newcat_1 = data.frame(Bwt = 4.0)
calc_pred_int(cat_model, newcat_1)
```

**(c)** After writing the function, run this code:

```{r}
newcat_2 = data.frame(Bwt = 3.3)
calc_pred_int(cat_model, newcat_2, level = 0.99)
```


